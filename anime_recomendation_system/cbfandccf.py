# -*- coding: utf-8 -*-
"""CBFandCCF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-VWTROI0DV4-6rXCPoBvYPW7SolFHf6g

#### Import Library
"""

import os
import zipfile
import shutil
import pandas as pd
import numpy as np
from sklearn.utils import shuffle
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
from sklearn.metrics import accuracy_score

""" Package ini memungkinkan untuk berinteraksi dengan API Kaggle dan mengunduh dataset dari Kaggle secara langsung melalui Colab."""

!pip install kaggle
from google.colab import files
files.upload()

!ls

os.environ['KAGGLE_CONFIG_DIR']='/content'

"""#### Download Dataset"""

!kaggle datasets download -d hernan4444/anime-recommendation-database-2020

data_dir =('/content/anime-recommendation-database-2020.zip')
zip_ref  = zipfile.ZipFile(data_dir,'r')
zip_ref.extractall('/content')
zip_ref.close()

"""## Data Understanding"""

anime = pd.read_csv('/content/anime.csv')
anime.head(5)

anime.info()

anime.describe()

rating_complete = pd.read_csv('/content/rating_complete.csv')
rating_complete.head(5)

max(rating_complete.user_id)

rating_complete.info()

len(rating_complete)

rating_complete.describe()

chunk_size = 10000
anime_list = pd.read_csv('/content/animelist.csv', chunksize=chunk_size)
for chunk in anime_list:
  print(chunk.head(5))
  break

anime_with_synopsis = pd.read_csv('/content/anime_with_synopsis.csv',chunksize=chunk_size)
for chunk in anime_with_synopsis:
  print(chunk.head(5))
  break

anime_with_synopsis = pd.read_csv('/content/anime_with_synopsis.csv')
anime_with_synopsis.head(5)

anime_with_synopsis.info()

anime_with_synopsis.isnull().sum()

anime_with_synopsis = anime_with_synopsis.dropna()

anime_with_synopsis.duplicated().sum()

anime_with_synopsis.reset_index(drop=True, inplace=True)

watching_status = pd.read_csv('/content/watching_status.csv', chunksize=chunk_size)
for chunk in watching_status:
  print(chunk.head(5))
  break

"""## Exploratory Data Analysis"""

anime_exp = anime.copy()

""" Beberapa kolom dalam dataset mungkin memiliki tipe data yang tidak sesuai dengan jenis data yang sebenarnya. Misalnya, kolom "Score" disimpan sebagai object, tetapi seharusnya berisi nilai numerik (int). Akibat dari hal ini, kita tidak bisa memvisualisasikan data dengan baik karena .corr dan .pairplot hanya berlaku untuk tipe data numerik. Oleh karena itu, perlu mengubah tipe data kolom tersebut menjadi tipe data numerik yang sesuai."""

anime_exp['Score'] = pd.to_numeric(anime_exp['Score'], errors='coerce')
anime_exp['Score'].fillna(0, inplace=True)
anime_exp['Score'] = anime_exp['Score'].astype(int)
anime_exp['Episodes'] = pd.to_numeric(anime_exp['Episodes'], errors='coerce')
anime_exp['Episodes'].fillna(0, inplace=True)
anime_exp['Episodes'] = anime_exp['Episodes'].astype(int)
anime_exp['Aired'] = anime_exp['Aired'].astype(str)
anime_exp['Aired'] = anime_exp['Aired'].str.extract(r'(\d{4})')
anime_exp['Aired'] = pd.to_numeric(anime_exp['Aired'], errors='coerce')
anime_exp['Aired'].fillna(0, inplace=True)
anime_exp['Aired'] = anime_exp['Aired'].astype(int)
anime_exp['Ranked'] = anime_exp['Ranked'].astype(str)
anime_exp['Ranked'] = anime_exp['Ranked'].str.replace(r'#', '')
anime_exp['Ranked'] = pd.to_numeric(anime_exp['Ranked'], errors='coerce')
anime_exp['Ranked'].fillna(0, inplace=True)
anime_exp['Ranked'] = anime_exp['Ranked'].astype(int)

sns.pairplot(anime_exp, diag_kind = 'kde')

selected_numerical_features = ['Aired','Episodes', 'Favorites', 'Members', 'Ranked', 'Popularity', 'Score']
selected_categorical_features = ['Type', 'Rating']

plt.figure(figsize=(10, 8))
correlation_matrix = anime_exp[selected_numerical_features].corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix For Numeric Feature ", size=20)

for feature in selected_categorical_features:
    plt.figure(figsize=(10, 6))
    sns.barplot(x=feature, y='Score', data=anime_exp)
    plt.xticks(rotation=45)
    plt.title(f'{feature} vs Score')
    plt.tight_layout()
    plt.show()

"""## Content Based Filtering

*Content-based filtering* adalah salah satu metode dalam sistem rekomendasi yang digunakan untuk memberikan rekomendasi kepada pengguna berdasarkan karakteristik atau konten dari item atau objek. Metode ini memeriksa atribut-atribut atau karakteristik dari item-item tersebut dan mencoba mencari kesamaan atau korelasi antara item dengan item lainnya berdasarkan atribut-atribut ini. Pada kasus ini, yang menjadi fitur untuk membuat model ini adalah fitur 'Name' dan 'Genres' dari dataset anime.
"""

new_data_anime = anime[['Name','Genres']]
new_data_anime.head()

print('Jumlah Genre anime: ', len(new_data_anime.Genres.unique()))
print('Unique Genre: ', new_data_anime.Genres.unique())

new_data_anime.info()

"""## Data Preparation"""

new_data_anime.isnull().sum()

new_data_anime = new_data_anime.dropna()

new_data_anime.duplicated().sum()

new_data_anime = new_data_anime.drop_duplicates()

print(len(new_data_anime))

new_data_anime.describe()

"""Dalam kode dibawah ini, reset_index(drop=True, inplace=True) digunakan untuk mereset indeks DataFrame setelah melakukan operasi dropna dan drop_duplicates. Dengan melakukan ini, kita memastikan bahwa indeks DataFrame adalah berurutan dari 0 hingga jumlah baris yang ada. Jika indeks tidak direset setelah operasi penghapusan baris, itu bisa menyebabkan perhitungan one-hot encoding yang salah. Contoh masalah sebelumnya yaitu, setelah one hot encode, jumlah rows kembali ke semula (sebelum drop)

## Model

MultiLabelBinarizer untuk mengubah masalah klasifikasi multi-label menjadi format yang lebih mudah dipahami oleh algoritma pembelajaran mesin, terutama dalam konteks klasifikasi biner. Ini melakukan binarisasi atau pemetaan label-label yang ada menjadi bentuk biner (0 atau 1) untuk setiap kelas atau label yang mungkin. Pada dataset ini, setiap jenis genres akan diubah menjadi 1 dan 0.
"""

from sklearn.preprocessing import MultiLabelBinarizer

new_data_anime['Genres'] = new_data_anime['Genres'].apply(lambda x: [genre.strip("[]'") for genre in x.split(', ')])

new_data_anime.reset_index(drop=True, inplace=True)

mlb = MultiLabelBinarizer()
encoded_genres = pd.DataFrame(mlb.fit_transform(new_data_anime['Genres']), columns=mlb.classes_)
encoded_data = pd.concat([new_data_anime[['Name', 'Genres']], encoded_genres], axis=1)
print(len(encoded_data))

encoded_data.head()

print(encoded_data.columns)

encoded_data.drop('Genres', axis=1)

"""*cousine similarity* bekerja dengan mengukur kesamaan antara anime digunakan metode cosine similarity pada vektor genre. Ketika seorang pengguna memilih anime favoritnya, sistem akan menghitung kesamaan antara anime tersebut dengan anime lain berdasarkan genre yang dimiliki."""

from sklearn.metrics.pairwise import cosine_similarity
cosine_sim = cosine_similarity(encoded_genres)
cosine_sim_df = pd.DataFrame(cosine_sim, index=new_data_anime['Name'], columns=new_data_anime['Name'])

def anime_recommendations(target_anime_title, similarity_data=cosine_sim_df, k=15):
    target_index = similarity_data.index.get_loc(target_anime_title)
    index = similarity_data.iloc[target_index].values.argsort()[-k-1:-1]
    closest = similarity_data.index[index]

    return closest

"""user_id = rating_complete['user_id'].unique().tolist()
print('list user_id: ', user_id)
user_to_user_encoded = {x: i for i, x in enumerate(user_id)}
user_encoded_to_user = {i: x for i, x in enumerate(user_id)}
"""

anime_title = 'Blue Seed'
recommendations = anime_recommendations(anime_title)
recommendations = recommendations[recommendations != anime_title]
recommendations_df = pd.DataFrame({'Recommended Anime': recommendations})
recommendations_df['Similarity Score'] = [cosine_sim_df.loc[anime_title, anime] for anime in recommendations]
recommendations_df = recommendations_df.sort_values(by=['Similarity Score', 'Recommended Anime'], ascending=[False, True])
recommendations_df.reset_index(drop=True, inplace=True)
recommendations_df.index += 1

recommendations_df

"""## Collaborative Filtering

*Collaborative filtering* adalah metode yang digunakan dalam sistem rekomendasi untuk memberikan rekomendasi kepada pengguna berdasarkan perilaku dan preferensi pengguna lainnya. Metode ini mengasumsikan bahwa pengguna yang memiliki preferensi atau perilaku serupa dalam interaksi dengan item-item akan memiliki preferensi yang mirip dalam item-item yang lainnya. Pada kasus ini, data rating_complete digunakan sebagai pembuatan model collaborative filtering.
"""

rating_complete.head(10)

print(rating_complete.shape)

rating_complete.isnull().sum()

rating_complete.duplicated().sum()

"""## Data Preparation"""

user_ids = rating_complete['user_id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

anime_ids = rating_complete['anime_id'].unique().tolist()
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

rating_complete['user'] = rating_complete['user_id'].map(user_to_user_encoded)

rating_complete['anime'] = rating_complete['anime_id'].map(anime_to_anime_encoded)

num_users = len(user_to_user_encoded)
print(num_users)

num_anime = len(anime_encoded_to_anime)
print(num_anime)

rating_complete['rating'] = rating_complete['rating'].values.astype(np.float32)

min_rating = min(rating_complete['rating'])
max_rating = max(rating_complete['rating'])

print('Number of User: {}, Number of Anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""### Split Data"""

# rating_complete = rating_complete.sample(frac=1, random_state=42)
# rating_complete

"""Karena ukuran dataset terlalu besar dan mengakibatkan crash pada colab, kita akan menggunakan 20% data."""

subset_size = int(0.2 * rating_complete.shape[0])
rating_complete_subset = rating_complete.sample(subset_size, random_state=42)
print(rating_complete_subset.shape)

rating_complete_subset_copy = rating_complete_subset.copy()

rating_complete_subset = shuffle(rating_complete_subset, random_state=42)
x = rating_complete_subset[['user', 'anime']].values

y = rating_complete_subset['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = rating_complete_subset.shape[0] - 10000
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print("Data x_train: ",x_train.shape)
print("Data x_val  : ",x_val.shape)
print("Data y_train: ",y_train.shape)
print("Data y_val  : ",y_val.shape)

"""## Model

TPUs adalah jenis unit pemrosesan khusus yang dirancang khusus untuk akselerasi tugas-tugas yang melibatkan pemrosesan tensor, seperti yang sering terjadi dalam pelatihan model deep learning. Keuntungan menggunakan TPU adalah kecepatan pemrosesan yang sangat tinggi, yang dapat mengurangi waktu yang dibutuhkan untuk melatih model deep learning secara signifikan dibandingkan dengan penggunaan CPU atau GPU konvensional. Pada kasus ini, TPU sangat dibutuhkan karena jumlah data yang sangat besar.
"""

try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Device:', tpu.master())
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
except Exception:
    strategy = tf.distribute.get_strategy()
    print('Using default strategy for CPU and single GPU')

"""RecommenderNet adalah model rekomendasi kolaboratif yang digunakan untuk memberikan rekomendasi anime kepada pengguna berdasarkan perilaku dan preferensi pengguna lainnya."""

with strategy.scope():
  class RecommenderNet(tf.keras.Model):

    def __init__(self, num_users, num_anime, embedding_size, **kwargs):
      super(RecommenderNet, self).__init__(**kwargs)
      self.num_users = num_users
      self.num_anime = num_anime
      self.embedding_size = embedding_size
      self.user_embedding = layers.Embedding(
          num_users,
          embedding_size,
          embeddings_initializer = 'he_normal',
          embeddings_regularizer = keras.regularizers.l2(1e-6)
      )
      self.user_bias = layers.Embedding(num_users, 1)
      self.anime_embedding = layers.Embedding(
          num_anime,
          embedding_size,
          embeddings_initializer = 'he_normal',
          embeddings_regularizer = keras.regularizers.l2(1e-6)
      )
      self.anime_bias = layers.Embedding(num_anime, 1)

    def call(self, inputs):
      user_vector = self.user_embedding(inputs[:,0])
      user_bias = self.user_bias(inputs[:, 0])
      anime_vector = self.anime_embedding(inputs[:, 1])
      anime_bias = self.anime_bias(inputs[:, 1])

      dot_user_anime = tf.tensordot(user_vector, anime_vector, 2)

      x = dot_user_anime + user_bias + anime_bias

      return tf.nn.sigmoid(x)

with strategy.scope():
  model = RecommenderNet(num_users, num_anime, 50)
  model.compile(
      loss = tf.keras.losses.BinaryCrossentropy(),
      optimizer = Adam(learning_rate=0.00001),
      metrics= [tf.keras.metrics.MeanAbsoluteError()]
      # metrics= tf.keras.metrics.RootMeanSquaredError()]
  )

with strategy.scope():
  history = model.fit(
      x = x_train,
      y = y_train,
      batch_size = 10000,
      epochs = 30,
      validation_data = (x_val, y_val)
  )

plt.plot(history.history['mean_absolute_error'])
plt.plot(history.history['val_mean_absolute_error'])
plt.title('model_metrics')
plt.ylabel('mean_absolute_error')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""Berdasarkan Grafik tersebut, loss menurun seiring epoch. Ini berarti model semakin baik dalam menyesuaikan diri dengan data pelatihan dan telah menemukan pola yang lebih baik dalam data tersebut. Namun, penting untuk diingat bahwa penurunan loss tidak selalu menjamin bahwa model sudah optimal. Diperlukan untuk memonitor metrik lain seperti akurasi atau metrik yang lebih sesuai dengan masalah untuk mendapatkan gambaran yang lebih lengkap tentang kinerja model.

## Testing Model
"""

anime_df = anime_with_synopsis
df = rating_complete_subset_copy

user_idf = df.user_id.sample(1).iloc[0]
anime_watched_by_user = df[df.user_id == user_idf]

anime_not_watched = anime_df[~anime_df['MAL_ID'].isin(anime_watched_by_user.anime_id.values)]['MAL_ID']
anime_not_watched = list(
    set(anime_not_watched)
    .intersection(set(anime_to_anime_encoded.keys()))
)

anime_not_watched = [[anime_to_anime_encoded.get(x)] for x in anime_not_watched]
user_encoder = user_to_user_encoded.get(user_idf)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched), anime_not_watched)
)

ratings = model.predict(user_anime_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_watched[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_idf))
print('===' * 9)
print('Anime with high ratings from user')
print('----' * 8)

top_anime_user = (
    anime_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

anime_df_rows = anime_df[anime_df['MAL_ID'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.Name, ':', row.Genres)

print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)

recommended_anime = anime_df[anime_df['MAL_ID'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.Name, ':', row.Genres)

"""## Matrik Evaluasi

Perhitungan beberapa metrik evaluasi untuk mengukur kualitas rekomendasi yang dihasilkan oleh model, termasuk Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), dan Mean Squared Error (MSE).
"""

ratings = model.predict(x_val).flatten()
true_ratings = y_val
mae = mean_absolute_error(true_ratings, ratings)
rmse = np.sqrt(mean_squared_error(true_ratings, ratings))
mse = mean_squared_error(true_ratings, ratings)

print("Mean Absolute Error (MAE):", mae)
print("Root Mean Squared Error (RMSE):", rmse)
print("Mean Squared Error (MSE):", mse)